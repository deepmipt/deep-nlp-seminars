{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# just for better visual performance\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to quickly refresh basic concepts of convolutional neural networks.\n",
    "It is necessary to answer these questions before you continue to go further.\n",
    "Firstly, try to answer the questions below and write your answers into cell below.\n",
    "If you wish, you may draw illustrations, put them into notebook and send all as an archive.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions you need to answer first\n",
    "\n",
    "0. What parameters are trainable in convolutional layers?\n",
    "* How many trainable weights there are in the single convolutional layer with kernel size [3x3],\n",
    "     where the input is an RGB image and number of filters is 16? \n",
    "* Is convolutional nn stable to resizing of the input data? (It is also necessary to explain `why?`) \n",
    "* What is `padding`?\n",
    "* What is the difference between `\"SAME\"` & `\"VALID\"` padding?\n",
    "* What does `strides` parameter affect?\n",
    "* What is a `pooling` layer in general? What is the difference between `max pooling` & `global max pooling`?\n",
    "* How to implement 1D convolution using 2D convolution function?\n",
    "* What is the `receptive field`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answers here:\n",
    "\n",
    "~just a lot of space for you~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Checking tips:\n",
    "Every right answer +1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some na√Øve examples of 2D convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets make 2 filters with 7x7 size and one input channel;\n",
    "\n",
    "from sklearn.datasets import load_sample_image\n",
    "china = load_sample_image(\"china.jpg\")\n",
    "flower = load_sample_image(\"flower.jpg\")\n",
    "image = china[150:220, 130:250]\n",
    "height, width, channels = image.shape\n",
    "image_grayscale = image.mean(axis=2).astype(np.float32)\n",
    "images = image_grayscale.reshape(1, height, width, 1)\n",
    "print(\"These are the filters that we will apply:\")\n",
    "\n",
    "\n",
    "fmap = np.zeros(shape=(7, 7, 1, 2), dtype=np.float32)\n",
    "fmap[:, 3, 0, 0] = 1 # mark column 3\n",
    "fmap[3, :, 0, 1] = 1 # mark row 3\n",
    "\n",
    "plt.imshow(fmap[:, :, 0, 0])\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.imshow(fmap[:, :, 0, 1])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And apply them with 2D convolutions,\n",
    "# without changing original image size\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, height, width, 1))\n",
    "feature_maps = tf.constant(fmap)\n",
    "convolution = tf.nn.conv2d(X, feature_maps, strides=[1,1,1,1], padding=\"SAME\", use_cudnn_on_gpu=False)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = convolution.eval(feed_dict={X: images})\n",
    "\n",
    "    \n",
    "# simply visualization code below\n",
    "\n",
    "fig = plt.figure(figsize=(36, 12))\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "plot_image(images[0, :, :, 0])\n",
    "ax.set_title(\"china_original\", )\n",
    "ax.grid('off')\n",
    "ax.axis('off')\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "plot_image(output[0, :, :, 0])\n",
    "ax.set_title(\"china_vertical\")\n",
    "ax.grid('off')\n",
    "ax.axis('off')\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "plot_image(output[0, :, :, 1])\n",
    "ax.set_title(\"china_horizontal\")\n",
    "ax.grid('off')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to explain why these images turned out in a such way after applying the horizontal and vertical filters.**\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "~right here, yes~\n",
    "\n",
    "---\n",
    "#### Checking tips:\n",
    "Right answer +2 points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the train data from https://www.kaggle.com/c/quora-question-pairs/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train/dev/test split\n",
    "\n",
    "Proportions are: 0.8, 0.1, 0.1 for train, validation, & test respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for n in tqdm(range(len(df))):\n",
    "    # str fixes some strange\n",
    "    q1 = str(df.question1[n])\n",
    "    q2 = str(df.question2[n])\n",
    "    toks_1 = word_tokenize(q1.lower())\n",
    "    toks_2 = word_tokenize(q2.lower())\n",
    "    counter.update(toks_1)\n",
    "    counter.update(toks_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_most_common_to_keep = 20000\n",
    "\n",
    "base_dict = {token: n + 1 for n, (token, _) in enumerate(counter.most_common()[:n_most_common_to_keep - 1])}\n",
    "\n",
    "# Dictionary with default value = 0\n",
    "tok2idx = defaultdict(int)\n",
    "tok2idx.update(base_dict)\n",
    "\n",
    "def toks2idxs(tokens):\n",
    "    return [tok2idx[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = len(df)\n",
    "np.random.seed(seed=42)\n",
    "\n",
    "# Shuffle the indices\n",
    "idxs = np.random.permutation(N)\n",
    "split_points = [int(0.8 * N), int(0.9 * N)]\n",
    "\n",
    "# shuffled dataset will be divided into 3 parts with sizes: [0.8*N, 0.1.*N, 0.1*N]\n",
    "train_idxs, dev_idxs, test_idxs = np.split(idxs, split_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(batch_size,\n",
    "                    data_type='train',\n",
    "                    shuffle=True,\n",
    "                    allow_smaller_last_batch=True):\n",
    "    \n",
    "    if data_type == 'train':\n",
    "        idxs = train_idxs\n",
    "    elif data_type == 'dev':\n",
    "        idxs = dev_idxs\n",
    "    elif data_type == 'test':\n",
    "        idxs = test_idxs\n",
    "\n",
    "    n_samples = len(idxs)\n",
    "    \n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            idxs = np.random.permutation(idxs)\n",
    "            \n",
    "        for k in range(n_batches):\n",
    "            batch_start = k * batch_size\n",
    "            batch_end = min((k + 1) * batch_size, n_samples)\n",
    "            current_batch_size = batch_end - batch_start\n",
    "            x1_utterance_list = []\n",
    "            x2_utterance_list = []\n",
    "            y_list = []\n",
    "            max_len_1 = 0\n",
    "            max_len_2 = 0\n",
    "            \n",
    "            for idx in idxs[batch_start: batch_end]:\n",
    "                question_1 = word_tokenize(str(df.question1[idx]).lower())\n",
    "                question_2 = word_tokenize(str(df.question2[idx]).lower())\n",
    "\n",
    "                x1_utterance_list.append(question_1)\n",
    "                x2_utterance_list.append(question_2)\n",
    "\n",
    "                y_list.append(df.is_duplicate[idx])\n",
    "                \n",
    "                # find the maximum length of sequence for current batch\n",
    "                max_len_1 = max(max_len_1, len(question_1))\n",
    "                max_len_2 = max(max_len_2, len(question_2))\n",
    "\n",
    "            # Fill in the data into numpy nd-arrays filled with padding indices\n",
    "            x_1 = np.zeros([current_batch_size, max_len_1], dtype=np.int32)\n",
    "            x_2 = np.zeros([current_batch_size, max_len_2], dtype=np.int32)\n",
    "            y = np.array(y_list)\n",
    "\n",
    "            for n in range(current_batch_size):\n",
    "                utt_len_1 = len(x1_utterance_list[n])\n",
    "                utt_len_2 = len(x2_utterance_list[n])\n",
    "\n",
    "                x_1[n, :utt_len_1] = toks2idxs(x1_utterance_list[n])\n",
    "                x_2[n, :utt_len_2] = toks2idxs(x2_utterance_list[n])\n",
    "\n",
    "            yield ({'input_1': x_1, 'input_2': x_2}, {'output': y})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quality metric in Quora competition\n",
    "\n",
    "`Log loss`, or `logistic loss`, or `cross-entropy loss` has been used.\n",
    "\n",
    "For a single sample with true label $y_t$ in {0,1} and estimated probability $y_p$ that $y_t = 1$, the `log loss` is\n",
    "\n",
    " $$log\\_loss = -(y_t \\log(y_p) + (1 - y_t) \\log(1 - y_p))$$\n",
    "\n",
    "So, the model minimize `binary_crossentropy`;\n",
    "\n",
    "----\n",
    "\n",
    "The code below implements the model which **was intentionally corrupted**;\n",
    "\n",
    "Try to improve the model.\n",
    "\n",
    "**Some hints:**\n",
    "* What type of activation is used?\n",
    "* How to reduce `covariance shift`?\n",
    "* MOAR LAYERS ~~~\n",
    "* There're hipster activation functions. O'RLY?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Conv1D, Concatenate, Dense, Input, GlobalMaxPool1D, SimpleRNN\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "embedding_dim = 64\n",
    "n_filters = 16\n",
    "kernel_width = 3\n",
    "learning_rate = 1e-0\n",
    "batch_size = 50\n",
    "\n",
    "emb_layer = keras.layers.Embedding(n_most_common_to_keep, embedding_dim)\n",
    "\n",
    "input_1 = Input([None], dtype=tf.float32, name='input_1')\n",
    "units_1 = emb_layer(input_1)\n",
    "\n",
    "input_2 = Input([None], dtype=tf.float32, name='input_2')\n",
    "units_2 = emb_layer(input_2)\n",
    "\n",
    "conv_layer = Conv1D(n_filters, kernel_width, padding='same')\n",
    "units_1 = conv_layer(units_1)\n",
    "units_2 = conv_layer(units_2)\n",
    "    \n",
    "units_1 = GlobalMaxPool1D()(units_1)\n",
    "units_2 = GlobalMaxPool1D()(units_2)\n",
    "\n",
    "# try to use matmul instead of concatenation just for experiment\n",
    "units = Concatenate()([units_1, units_2])\n",
    "predictions = Dense(1, activation='sigmoid', name='output')(units)\n",
    "\n",
    "model = Model(inputs=[input_1, input_2], outputs=predictions)\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture visualization\n",
    "\n",
    "To plot Keras model graph `graphviz` and `pydot` packages must be installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also `graphviz` should be installed;\n",
    "\n",
    "If you are _Linux_ user, it is better to do this via your package manager;\n",
    "\n",
    "For _Ubuntu_ users the command should look like this:\n",
    "        `sudo apt-get install graphviz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_layer_names=True, show_shapes=False).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_num = 1\n",
    "hist = model.fit_generator(batch_generator(batch_size, data_type='train'),\n",
    "                           steps_per_epoch=1000,\n",
    "                           epochs=epoch_num,\n",
    "                           validation_data=batch_generator(batch_size, data_type='dev'),\n",
    "                           validation_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(hist.history['loss'], label = 'train_loss')\n",
    "plt.plot(hist.history['val_loss'], label = 'val_loss')\n",
    "plt.plot(hist.history['acc'], label = 'train_acc')\n",
    "plt.plot(hist.history['val_acc'], label = 'val_acc')\n",
    "plt.legend(fontsize=13)\n",
    "plt.title(\"Train dynamic\")\n",
    "plt.xlabel(\"Epoch number\")\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also easy to save picture on the disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model=model, to_file=\"lol.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model performance on the last part of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# it takes time to make a prediction\n",
    "%time result = model.evaluate_generator(batch_generator(batch_size, data_type='test'),\n",
    "                                        steps=int(0.1*N)//batch_size)\n",
    "\n",
    "print('\\nlogloss: ', result[0])\n",
    "print('accuracy: ', result[1])\n",
    "\n",
    "assert result[0]<=0.485, \"It seems the result is worse then baseline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "The task is just to improve the model and get maximum points for practical part\n",
    "\n",
    "* If your logloss on test is lower than 1, you got **5 points**\n",
    "* If lower then 0.5 --> you got **7 points**\n",
    "* If you have reached the result logloss lower then 0.3, then you got **10 points**\n",
    "\n",
    "\n",
    "! *If a person has used batchnorm __right after the activation__, she/he lose 1 point for final mark!*\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Overall\n",
    "\n",
    "For that Homework your mark is calculated simply:\n",
    "$$0.5\\cdot(theoretical\\_part\\_points + practical\\_part\\_points)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
